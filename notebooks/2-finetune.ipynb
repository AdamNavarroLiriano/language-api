{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sacrebleu\n",
    "! pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import transformers\n",
    "import os \n",
    "from datasets import load_dataset, load_metric\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from sacrebleu import sentence_bleu\n",
    "from functools import partial\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"]=\"true\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(language_pair: tuple[str]) -> dict:\n",
    "    \"\"\"Loads pretrained models from MarianMT\n",
    "\n",
    "    :param language_pair: tuple containing src language and tgt language\n",
    "    :type language_pair: tuple[str]\n",
    "    :param cache_path: path to save cache for loading models\n",
    "    :type cache_path: str\n",
    "    :return: dictionary containing tokenizer and model objects\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    # Get src and tgt language pairs\n",
    "    src, tgt = language_pair\n",
    "    model_name = f\"Helsinki-NLP/opus-mt-{src}-{tgt}\"\n",
    "\n",
    "    # Load from huggingface or cache\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "    return {\"tokenizer\": tokenizer, \"model\": model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_pair = 'en_sv'\n",
    "src, tgt = language_pair.split('_')\n",
    "\n",
    "train = pd.read_parquet(f'/kaggle/input/en-da-dataset/train/{language_pair}.parquet')\n",
    "valid = pd.read_parquet(f'/kaggle/input/en-da-dataset/valid/{language_pair}.parquet')\n",
    "test = pd.read_parquet(f'/kaggle/input/en-da-dataset/test/{language_pair}.parquet')\n",
    "\n",
    "sample_size = train.shape[0]\n",
    "train_sample = train.sample(n=sample_size, random_state=12)\n",
    "\n",
    "data = datasets.DatasetDict()\n",
    "data['train'] = datasets.Dataset.from_list(train_sample['translation'].tolist(), split='train')\n",
    "data['valid'] = datasets.Dataset.from_list(valid['translation'].tolist(), split='valid')\n",
    "data['test'] = datasets.Dataset.from_list(test['translation'].tolist(), split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = f\"Helsinki-NLP/opus-mt-{src}-{tgt}\"\n",
    "tokenizer_en_sv = MarianTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"P98 for source text: {train['source_text'].str.len().quantile(0.98)}\")\n",
    "print(f\"P98 for target text: {train['target_text'].str.len().quantile(0.98)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 256\n",
    "max_target_length = 256\n",
    "def preprocess_function(sentence, tokenizer, src, tgt, max_input_length, max_target_length):\n",
    "    inputs = [pair[src] for pair in sentence[\"translation\"]]\n",
    "    targets = [pair[tgt] for pair in sentence[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    \n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_es_sv = partial(preprocess_function, \n",
    "                           tokenizer=tokenizer_en_sv, \n",
    "                              src=src, \n",
    "                              tgt=tgt, \n",
    "                              max_input_length=max_input_length, \n",
    "                              max_target_length=max_target_length\n",
    "                          )\n",
    "tokenized_datasets = data.map(preprocess_es_sv, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_en_sv = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_epochs = 3\n",
    "learning_rate = 2e-5\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{src}-{tgt}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=train_epochs,\n",
    "    predict_with_generate=True    \n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer_en_sv, model=model_en_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model_en_sv,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer_en_sv,\n",
    "    compute_metrics=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f'/kaggle/working/finetuned-mt-{src}-{tgt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute BLEU on finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tuned_model_name = f'/kaggle/working/finetuned-mt-{src}-{tgt}'\n",
    "tuned_tokenizer = MarianTokenizer.from_pretrained(tuned_model_name)\n",
    "tuned_model = MarianMTModel.from_pretrained(tuned_model_name).to(device)\n",
    "tuned_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T13:38:13.805537Z",
     "iopub.status.busy": "2024-06-01T13:38:13.805150Z",
     "iopub.status.idle": "2024-06-01T13:38:13.809906Z",
     "shell.execute_reply": "2024-06-01T13:38:13.808894Z",
     "shell.execute_reply.started": "2024-06-01T13:38:13.805507Z"
    }
   },
   "source": [
    "Using the test dataset, let's compute BLEU score between the pretrained model and our finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "test_sample = test.copy()\n",
    "test_source = test_sample['source_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(src_text, batch_size, tokenizer, model):\n",
    "    tokenized_text = [tokenizer(src_text[i:(i+batch_size)],\n",
    "                                return_tensors=\"pt\", padding=True).to(device)\n",
    "                      for i in range(0, len(src_text), batch_size)\n",
    "                     ]\n",
    "\n",
    "    # Get translations\n",
    "    translations =  [model.generate(**batch) for batch in tokenized_text]\n",
    "    decoded_translations = [tokenizer.decode(text, skip_special_tokens=True) \n",
    "                            for batch in translations for text in batch]\n",
    "    return decoded_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_results = translate_text(test_source, batch_size, tuned_tokenizer, tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_finetuned = test_sample.copy()\n",
    "test_finetuned['prediction'] = finetuned_results\n",
    "test_finetuned['bleu'] = (test_finetuned\n",
    "                          .apply(lambda x: sentence_bleu(x['prediction'],\n",
    "                                                         [x['target_text']]).score, \n",
    "                                 axis=1)\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BLEU score on finetuned model: ', test_finetuned['bleu'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute BLEU score on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sv_pretrained_model = load_models(('en', 'sv'))\n",
    "model_pretr_en_sv = en_sv_pretrained_model['model'].to(device)\n",
    "tokenizer_pretr_en_sv = en_sv_pretrained_model['tokenizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_translations = translate_text(test_source, batch_size, \n",
    "                                         tokenizer_pretr_en_sv, model_pretr_en_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_finetuned['pretrained_predictions'] = pretrained_translations\n",
    "test_finetuned['pretrained_bleu'] = (test_finetuned\n",
    "                          .apply(lambda x: sentence_bleu(x['pretrained_predictions'],\n",
    "                                                         [x['target_text']]).score, \n",
    "                                 axis=1)\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_finetuned['pretrained_bleu'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_finetuned.to_csv('en_sv_finetuned_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r file.zip /kaggle/working/finetuned-mt-en-sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'file.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future: give it more epochs. Maybe sample data?"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5089032,
     "sourceId": 8577160,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
