{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import transformers\n",
    "\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import time\n",
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/kaggle/input/translations/translation_merged.parquet'\n",
    "translations_df = pd.read_parquet(data_path)\n",
    "language_pairs = translations_df[['src_lang', 'tgt_lang']].drop_duplicates()\n",
    "language_pairs = language_pairs.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load language translations models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(language_pair: tuple[str]) -> dict:\n",
    "    \"\"\"Loads pretrained models from MarianMT\n",
    "\n",
    "    :param language_pair: tuple containing src language and tgt language\n",
    "    :type language_pair: tuple[str]\n",
    "    :param cache_path: path to save cache for loading models\n",
    "    :type cache_path: str\n",
    "    :return: dictionary containing tokenizer and model objects\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    # Get src and tgt language pairs\n",
    "    src, tgt = language_pair\n",
    "    model_name = f\"Helsinki-NLP/opus-mt-{src}-{tgt}\"\n",
    "\n",
    "    # Load from huggingface or cache\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "    return {\"tokenizer\": tokenizer, \"model\": model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_models = {}\n",
    "for src, tgt in language_pairs:\n",
    "    print(f'Fetching {src}-{tgt} model')\n",
    "    \n",
    "    try:\n",
    "        mt_models[f'{src}_{tgt}'] = load_models((src, tgt))\n",
    "    except:\n",
    "        try:\n",
    "            src_mod = 'no' if src == 'nb'else src\n",
    "            tgt_mod = 'no' if tgt == 'nb' else tgt\n",
    "            mt_models[f'{src}_{tgt}'] = load_models((src_mod, tgt_mod))\n",
    "        except:\n",
    "            print(f'Model for {src}_{tgt} not found.')\n",
    "            mt_models[f'{src}_{tgt}'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models not supported\n",
    "models_not_supported = [key for key, value in mt_models.items() if value is None]\n",
    "models_not_supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is large, for each language pair we sample about 5K translations. We will compute statistics for the BLEU metric, using bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 5_000\n",
    "random_state = 12\n",
    "\n",
    "translations_df['lang_pair'] = translations_df['src_lang'] + '_' + translations_df['tgt_lang']\n",
    "translations_filtered_df = translations_df.loc[~translations_df['lang_pair'].isin(models_not_supported)]\n",
    "translations_filtered_df = translations_filtered_df.reset_index(drop=True)\n",
    "\n",
    "# Sample by each language pair\n",
    "translations_filtered_sample = (translations_filtered_df\n",
    "                                .groupby('lang_pair')\n",
    "                                .sample(n=sample_size, random_state=random_state)\n",
    "                                .reset_index(drop=True)\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lang_pair_dataframe(translations:pd.DataFrame, lang_pair:str) -> pd.DataFrame:\n",
    "    language_pair_df = (translations\n",
    "                    .loc[translations['lang_pair']==lang_pair]\n",
    "                    .reset_index(drop=True)\n",
    "                   )\n",
    "    return language_pair_df\n",
    "\n",
    "lang_pair_samples = {lang_pair: create_lang_pair_dataframe(translations_filtered_sample, lang_pair)\n",
    "                    for lang_pair in translations_filtered_sample['lang_pair'].unique()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we tokenize each src_text from the sample dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lang_pair_dataframe(lang_pair_df:pd.DataFrame, lang_pair: str, tokenizer, batch_size:int):\n",
    "    src_text = lang_pair_df['src_text'].tolist()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    tokenized_text = [tokenizer(src_text[i:(i+batch_size)], return_tensors=\"pt\", padding=True).to(device) \n",
    "                     for i in range(0, len(src_text), batch_size)]\n",
    "    \n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we produce the translation, and add it to the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_src_text(lang_pair_df, lang_pair, model, tokenized_text):\n",
    "    language_pair_df = lang_pair_df.copy()\n",
    "    model.eval()\n",
    "    \n",
    "    # Make translations\n",
    "    translated_text = []\n",
    "    t1 = time.time()\n",
    "    for i, batch in enumerate(tokenized_text):\n",
    "        translated = model.generate(**batch)\n",
    "        translated_batch = [\n",
    "            tokenizer.decode(t, skip_special_tokens=True) for t in translated\n",
    "        ]\n",
    "        translated_text.append(translated_batch)\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(f\"{i} done\")\n",
    "\n",
    "    t2 = time.time()\n",
    "    \n",
    "    print(f'Translation done in {(t2-t1)/60} mins')\n",
    "    \n",
    "    # Create translation column and compute BLEU score\n",
    "    language_pair_df['translation'] = [t for batch in translated_text for t in batch]\n",
    "    language_pair_df['bleu'] = language_pair_df.apply(lambda x: sacrebleu.sentence_bleu(x['translation'].replace('\"', '').strip(),\n",
    "                                                                                    [x['tgt_text'].replace('\"', '')]).score,axis=1)\n",
    "    \n",
    "    return language_pair_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "output_dir = \"/kaggle/working\"\n",
    "\n",
    "for lang_pair in lang_pair_list:\n",
    "    print(f'translating {lang_pair}')\n",
    "    df = lang_pair_samples[lang_pair]\n",
    "    \n",
    "    # Get model, modifying for Norwegian\n",
    "    src, tgt = lang_pair.split('_')\n",
    "    src_mod = 'no' if src == 'nb' else src\n",
    "    tgt_mod = 'no' if tgt == 'nb' else tgt\n",
    "    \n",
    "    model_name = f\"Helsinki-NLP/opus-mt-{src_mod}-{tgt_mod}\"\n",
    "\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokenized_text = tokenize_lang_pair_dataframe(df, lang_pair, tokenizer, batch_size)\n",
    "    result = translate_src_text(df, lang_pair, model, tokenized_text)\n",
    "    \n",
    "    # Save\n",
    "    result.to_csv(f'{output_dir}/{lang_pair}_bleu_sample.csv', index=False)\n",
    "    print(f\"{lang_pair} BLEU sample saved with mean {result['bleu'].mean()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5104494,
     "sourceId": 8543764,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
